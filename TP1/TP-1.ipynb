{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03061238,  0.04856196, -0.00865975, -0.01046024], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace d'action : Discrete(2)\n",
      "Espace d'observation : Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "Action : 1, Observation : [ 0.03158361  0.24380703 -0.00886896 -0.30586278], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03645976  0.43905422 -0.01498621 -0.6013295 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04524084  0.2441451  -0.0270128  -0.3134044 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05012374  0.43964124 -0.03328089 -0.61448246], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05891657  0.635212   -0.04557054 -0.91745895], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.07162081  0.44073483 -0.06391972 -0.6394391 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.08043551  0.63668704 -0.0767085  -0.9515467 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.09316924  0.44267654 -0.09573944 -0.683917  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.10202277  0.24900514 -0.10941777 -0.42284468], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.10700288  0.4454933  -0.11787467 -0.7479191 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.11591274  0.2521777  -0.13283305 -0.4945328 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.12095629  0.05915457 -0.14272371 -0.24648689], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.12213939 -0.13367116 -0.14765345 -0.00200764], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.11946597  0.06322588 -0.1476936  -0.33739212], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.12073048  0.260107   -0.15444145 -0.6727629 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.12593262  0.0674306  -0.1678967  -0.43241376], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.12728123 -0.12496582 -0.17654498 -0.19700725], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.12478192  0.07218414 -0.18048511 -0.5397706 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.1262256   0.26932284 -0.19128053 -0.8834504 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.13161206  0.07724094 -0.20894954 -0.65647656], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.13315688  0.2745646  -0.22207907 -1.0070093 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00081391 -0.17097966  0.03419575  0.3367504 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.0042335   0.02363939  0.04093076  0.05504407], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00376071  0.21815129  0.04203164 -0.22444911], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00060231  0.41264808  0.03754266 -0.50358295], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.00885527  0.21701768  0.02747099 -0.1993092 ], Reward : 1.0\n",
      "Action : 0, Observation : [0.01319563 0.0215138  0.02348481 0.10191142], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.0136259  -0.17393671  0.02552304  0.40191025], Reward : 1.0\n",
      "Action : 1, Observation : [0.01014717 0.02081411 0.03356124 0.11738212], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01056345  0.21543951  0.03590889 -0.16452645], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01487224  0.41002953  0.03261836 -0.44566834], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02307283  0.21446165  0.02370499 -0.14288448], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.02736207  0.40923622  0.0208473  -0.42799565], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03554679  0.60405684  0.01228739 -0.7140344 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.04762793  0.7990065  -0.0019933  -1.0028244 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.06360806  0.6039113  -0.02204979 -0.71076816], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07568628  0.79933155 -0.03626515 -1.0103095 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.09167291  0.99491817 -0.05647134 -1.3141562 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.11157127  0.80055463 -0.08275446 -1.0396693 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.12758237  0.6066238  -0.10354785 -0.7740707 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.13971485  0.8030063  -0.11902926 -1.0974553 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.15577497  0.6096353  -0.14097837 -0.8443631 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.16796768  0.80637044 -0.15786563 -1.1778486 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.18409508  1.0031503  -0.1814226  -1.515568  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.2041581   0.8106278  -0.21173395 -1.2845699 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02506344  0.15570508 -0.01000205 -0.28824192], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02194934  0.3509682  -0.01576689 -0.5840625 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01492998  0.15607065 -0.02744814 -0.2963878 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01180856  0.35157293 -0.03337589 -0.5975995 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00477711  0.5471456  -0.04532788 -0.90060586], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00616581  0.7428515  -0.06334    -1.2071849 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.02102284  0.54860246 -0.0874837  -0.9350052 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03199489  0.7447885  -0.1061838  -1.2538464 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.04689066  0.5511746  -0.13126072 -0.9962205 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05791415  0.7477843  -0.15118514 -1.3270781 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07286984  0.94445634 -0.1777267  -1.6630003 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.09175896  1.141147   -0.2109867  -2.005366  ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.03083705  0.239621    0.04692608 -0.29395244], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02604463  0.4340436   0.04104703 -0.5714741 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01736376  0.2383708   0.02961755 -0.2661477 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01259634  0.04283894  0.0242946   0.03572776], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01173956  0.23760423  0.02500916 -0.24919204], Reward : 1.0\n",
      "Action : 1, Observation : [-0.00698748  0.43236026  0.02002531 -0.5338828 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.00165973  0.62719494  0.00934766 -0.82018924], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.01420363  0.8221877  -0.00705613 -1.1099174 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.03064738  1.0174017  -0.02925448 -1.4048055 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05099541  1.2128744  -0.05735059 -1.7064887 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.07525291  1.4086071  -0.09148036 -2.016457  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.10342505  1.214546   -0.1318095  -1.7534401 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.12771596  1.0211427  -0.1668783  -1.5044917 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.14813882  1.2178499  -0.19696814 -1.8442882 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.17249581  1.0253702  -0.2338539  -1.6186862 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00136193 -0.20220546 -0.01997157  0.30089813], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00540604 -0.39703715 -0.0139536   0.58721614], Reward : 1.0\n",
      "Action : 0, Observation : [-0.01334678 -0.5919609  -0.00220928  0.8754711 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.025186   -0.396809    0.01530014  0.58209443], Reward : 1.0\n",
      "Action : 1, Observation : [-0.03312218 -0.20190473  0.02694203  0.29427028], Reward : 1.0\n",
      "Action : 0, Observation : [-0.03716027 -0.39740023  0.03282743  0.5953271 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.04510828 -0.59296584  0.04473398  0.8981671 ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.0569676  -0.7886647   0.06269731  1.204569  ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.07274089 -0.9845385   0.0867887   1.5162233 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.09243166 -0.7905672   0.11711317  1.2518454 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.108243   -0.5971241   0.14215007  0.99802095], Reward : 1.0\n",
      "Action : 0, Observation : [-0.12018549 -0.7938308   0.1621105   1.3317565 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.1360621  -0.6010814   0.18874562  1.093875  ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.14808373 -0.7981198   0.21062312  1.439346  ], Reward : 1.0\n",
      "Action : 0, Observation : [-0.02186752 -0.23902059  0.02116365  0.2919976 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02664793 -0.04420668  0.0270036   0.00606384], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02753207  0.1505178   0.02712488 -0.2779783 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.02452171  0.3452425   0.02156531 -0.5619842 ], Reward : 1.0\n",
      "Action : 1, Observation : [-0.01761686  0.5400553   0.01032563 -0.84779567], Reward : 1.0\n",
      "Action : 0, Observation : [-0.00681575  0.344794   -0.00663029 -0.5518837 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 8.012699e-05  5.400085e-01 -1.766796e-02 -8.466482e-01], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.0108803   0.73536694 -0.03460092 -1.1448343 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.02558764  0.93092334 -0.05749761 -1.448164  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.0442061   0.73655355 -0.0864609  -1.1739861 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.05893717  0.9326862  -0.10994062 -1.492474  ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.0775909   0.7390604  -0.13979009 -1.2360462 ], Reward : 1.0\n",
      "Action : 1, Observation : [ 0.0923721   0.93567413 -0.16451102 -1.5690545 ], Reward : 1.0\n",
      "Action : 0, Observation : [ 0.11108559  0.74285424 -0.19589211 -1.3318781 ], Reward : 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Espace d'action : {env.action_space}\")\n",
    "print(f\"Espace d'observation : {env.observation_space}\")\n",
    "\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "    print(f\"Action : {action}, Observation : {observation}, Reward : {reward}\")\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essai 1\n",
      "Action choisie : 1\n",
      "Nouvelle observation : [-0.01067524  0.19800374  0.01795238 -0.31931168]\n",
      "Récompense obtenue : 1.0\n",
      "Épisode terminé ? False\n",
      "--------------------------------------------------\n",
      "Essai 2\n",
      "Action choisie : 0\n",
      "Nouvelle observation : [-0.00671517  0.00263077  0.01156614 -0.02102174]\n",
      "Récompense obtenue : 1.0\n",
      "Épisode terminé ? False\n",
      "--------------------------------------------------\n",
      "Essai 3\n",
      "Action choisie : 0\n",
      "Nouvelle observation : [-0.00666255 -0.19265513  0.01114571  0.27528787]\n",
      "Récompense obtenue : 1.0\n",
      "Épisode terminé ? False\n",
      "--------------------------------------------------\n",
      "Essai 4\n",
      "Action choisie : 0\n",
      "Nouvelle observation : [-0.01051565 -0.3879343   0.01665146  0.57146525]\n",
      "Récompense obtenue : 1.0\n",
      "Épisode terminé ? False\n",
      "--------------------------------------------------\n",
      "Essai 5\n",
      "Action choisie : 1\n",
      "Nouvelle observation : [-0.01827434 -0.19304977  0.02808077  0.28407425]\n",
      "Récompense obtenue : 1.0\n",
      "Épisode terminé ? False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, _ = env.reset()\n",
    "\n",
    "for i in range(5):  # Essayer 5 actions différentes\n",
    "    action = env.action_space.sample()  # Action aléatoire\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    print(f\"Essai {i+1}\")\n",
    "    print(f\"Action choisie : {action}\")\n",
    "    print(f\"Nouvelle observation : {observation}\")\n",
    "    print(f\"Récompense obtenue : {reward}\")\n",
    "    print(f\"Épisode terminé ? {done}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, _ = env.reset()\n",
    "\n",
    "episode_length = 0  # Compteur de durée\n",
    "\n",
    "while True:\n",
    "    action = input(\"Entrez une action (0 = Gauche, 1 = Droite, Q = Quitter) : \")\n",
    "\n",
    "    if action.lower() == 'q':\n",
    "        break  # Quitter l'application\n",
    "\n",
    "    try:\n",
    "        action = int(action)\n",
    "        if action not in [0, 1]:\n",
    "            raise ValueError(\"Action invalide\")\n",
    "    except ValueError:\n",
    "        print(\" Veuillez entrer 0 ou 1\")\n",
    "        continue\n",
    "\n",
    "    # Appliquer l'action\n",
    "    observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "    # Afficher les nouvelles valeurs\n",
    "    print(f\"Observation : {observation}, Récompense : {reward}\")\n",
    "\n",
    "    episode_length += 1\n",
    "\n",
    "    if done:\n",
    "        print(f\" Épisode terminé après {episode_length} actions.\")\n",
    "        observation, _ = env.reset()\n",
    "        episode_length = 0  # Réinitialiser le compteur\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "num_episodes = 10\n",
    "episode_durations = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation, _ = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()  # Action aléatoire\n",
    "        observation, reward, done, truncated, _ = env.step(action)\n",
    "        step_count += 1\n",
    "\n",
    "    episode_durations.append(step_count)\n",
    "    print(f\"Épisode {episode+1} terminé en {step_count} étapes.\")\n",
    "\n",
    "# Calcul de la durée moyenne\n",
    "avg_duration = np.mean(episode_durations)\n",
    "print(f\" Durée moyenne sur {num_episodes} épisodes : {avg_duration:.2f} étapes\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
